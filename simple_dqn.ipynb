{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from environments import WordleEnv, WordleEnvMarkov\n",
    "def nested_list_to_tuple(nested_list):\n",
    "    return tuple(nested_list_to_tuple(i) if isinstance(i, list) else i for i in nested_list)\n",
    "\n",
    "# -----------------------------\n",
    "# Q-Learning Agent Definition\n",
    "# -----------------------------\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "def train_q_learning(env, num_episodes=1000, alpha=0.5, gamma=0.9, epsilon=0.2, log_dir=\"./logs/wordle_standard_q\"):\n",
    "    \"\"\"\n",
    "    A simple tabular Q-learning algorithm that trains on the Wordle environment with TensorBoard logging.\n",
    "    \n",
    "    The state is defined as a tuple: (attempt_number, board_state)\n",
    "    where board_state is a tuple of length word_length.\n",
    "    \n",
    "    This version prevents the agent from guessing the same word twice in a single episode.\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    \n",
    "    Q = {}\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    win_rate_window = deque(maxlen=100)\n",
    "    q_values_history = []\n",
    "    exploration_rates = []\n",
    "    unique_states_count = []\n",
    "    \n",
    "    epsilon_start = epsilon\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(observation):\n",
    "        \"\"\"Extract information state from observation\"\"\"\n",
    "        attempt = observation[\"attempt\"].item()\n",
    "        if \"board\" in observation and observation[\"board\"] is not None:\n",
    "            board = nested_list_to_tuple(observation)\n",
    "        else:\n",
    "            board = tuple()\n",
    "            \n",
    "        if attempt == 0:\n",
    "            return \"initial\"\n",
    "        return (attempt, board)\n",
    "    \n",
    "    def choose_action(state, guessed_actions, current_epsilon):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "        \n",
    "        if not allowed_actions:\n",
    "            return env.action_space.sample(), True\n",
    "            \n",
    "        if random.random() < current_epsilon or state not in Q:\n",
    "            return random.choice(allowed_actions), True\n",
    "        else:\n",
    "            q_values = Q[state]\n",
    "            allowed_q = {a: q_values[a] for a in allowed_actions}\n",
    "            return max(allowed_q, key=allowed_q.get), False\n",
    "    \n",
    "    def update_Q(state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-learning update rule\"\"\"\n",
    "        if state not in Q:\n",
    "            Q[state] = {a: 0 for a in range(env.action_space.n)}\n",
    "        if not done and next_state not in Q:\n",
    "            Q[next_state] = {a: 0 for a in range(env.action_space.n)}\n",
    "            \n",
    "        best_next = max(Q[next_state].values()) if not done else 0\n",
    "        Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])\n",
    "        \n",
    "        return Q[state][action]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observation, _ = env.reset()\n",
    "        state = get_state(observation)\n",
    "        guessed_actions = set() \n",
    "        done = False\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        exploration_count = 0\n",
    "        \n",
    "        current_epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "        \n",
    "        episode_q_values = []\n",
    "        \n",
    "        while not done:\n",
    "            action, is_exploration = choose_action(state, guessed_actions, current_epsilon)\n",
    "            guessed_actions.add(action)\n",
    "            if is_exploration:\n",
    "                exploration_count += 1\n",
    "            \n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = get_state(next_observation)\n",
    "            \n",
    "            new_q_value = update_Q(state, action, reward, next_state, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_step += 1\n",
    "            episode_q_values.append(new_q_value)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_step)\n",
    "        win_rate_window.append(1 if env.won else 0)\n",
    "        exploration_rates.append(exploration_count / episode_step if episode_step > 0 else 0)\n",
    "        \n",
    "        avg_q_value = sum(episode_q_values) / len(episode_q_values) if episode_q_values else 0\n",
    "        q_values_history.append(avg_q_value)\n",
    "        \n",
    "        unique_states_count.append(len(Q))\n",
    "        \n",
    "        writer.add_scalar('Metrics/Reward', episode_reward, episode)\n",
    "        writer.add_scalar('Metrics/Episode_Length', episode_step, episode)\n",
    "        writer.add_scalar('Metrics/Win_Rate', sum(win_rate_window) / len(win_rate_window), episode)\n",
    "        writer.add_scalar('Metrics/Exploration_Rate', exploration_rates[-1], episode)\n",
    "        writer.add_scalar('Metrics/Average_Q_Value', avg_q_value, episode)\n",
    "        writer.add_scalar('Metrics/Unique_States', unique_states_count[-1], episode)\n",
    "        writer.add_scalar('Hyperparameters/Epsilon', current_epsilon, episode)\n",
    "        writer.add_scalar('Hyperparameters/Learning_Rate', alpha, episode)\n",
    "        \n",
    "        if hasattr(env, 'won') and env.won and hasattr(env, 'target_word') and episode % 50 == 0:\n",
    "            writer.add_text('Examples/Won_Games', \n",
    "                           f\"Episode {episode}: Solved '{env.target_word}' in {episode_step} attempts\", \n",
    "                           episode)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            win_rate = sum(win_rate_window) / len(win_rate_window) if win_rate_window else 0\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | Reward: {episode_reward:.2f} | \"\n",
    "                  f\"Steps: {episode_step} | Win Rate: {win_rate:.2f} | \"\n",
    "                  f\"Epsilon: {current_epsilon:.4f} | Q-states: {len(Q)}\")\n",
    "    \n",
    "    writer.add_histogram('Histograms/Episode_Rewards', np.array(episode_rewards), 0)\n",
    "    writer.add_histogram('Histograms/Episode_Lengths', np.array(episode_lengths), 0)\n",
    "    writer.add_histogram('Histograms/Q_Values', np.array(q_values_history), 0)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Testing the Trained Agent with TensorBoard logging\n",
    "# -----------------------------\n",
    "def test_agent(env, Q, log_dir=\"./logs/wordle_standard_test\"):\n",
    "    \"\"\"Test the trained agent and log results to TensorBoard\"\"\"\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    \n",
    "    observation, _ = env.reset()\n",
    "    target_word = env.target_word if hasattr(env, 'target_word') else \"Unknown\"\n",
    "    \n",
    "    state = get_state(observation)\n",
    "    guessed_actions = set()\n",
    "    done = False\n",
    "    \n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    guesses = []\n",
    "    \n",
    "    print(\"\\nTesting trained agent:\")\n",
    "    while not done:\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "        if not allowed_actions:\n",
    "            action = env.action_space.sample()\n",
    "        elif state in Q:\n",
    "            allowed_q = {a: Q[state][a] for a in allowed_actions if a in Q[state]}\n",
    "            if allowed_q:\n",
    "                action = max(allowed_q, key=allowed_q.get)\n",
    "            else:\n",
    "                action = random.choice(allowed_actions)\n",
    "        else:\n",
    "            action = random.choice(allowed_actions)\n",
    "        \n",
    "        guessed_actions.add(action)\n",
    "        \n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        word = env.valid_words[action] if hasattr(env, 'valid_words') else f\"Action_{action}\"\n",
    "        board = observation['board'] if 'board' in observation else None\n",
    "        \n",
    "        guesses.append(word)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        writer.add_scalar('Test/Step_Reward', reward, step)\n",
    "        \n",
    "        print(f\"Guess: {word}, Board: {board}, Reward: {reward}\")\n",
    "        \n",
    "        state = get_state(observation)\n",
    "    \n",
    "    success = hasattr(env, 'won') and env.won\n",
    "    writer.add_scalar('Test/Total_Reward', total_reward, 0)\n",
    "    writer.add_scalar('Test/Steps', step, 0)\n",
    "    writer.add_scalar('Test/Success', 1 if success else 0, 0)\n",
    "    \n",
    "    summary = f\"Target word: {target_word}\\n\"\n",
    "    summary += f\"Success: {success}\\n\"\n",
    "    summary += f\"Steps: {step}\\n\"\n",
    "    summary += f\"Guesses: {', '.join(guesses)}\\n\"\n",
    "    summary += f\"Total reward: {total_reward}\"\n",
    "    \n",
    "    writer.add_text('Test/Game_Summary', summary, 0)\n",
    "    \n",
    "    if hasattr(env, 'render'):\n",
    "        env.render()\n",
    "    \n",
    "    writer.close()\n",
    "    return step, success, total_reward\n",
    "\n",
    "def get_state(observation):\n",
    "    \"\"\"Helper function to extract state from observation (used in testing)\"\"\"\n",
    "    attempt = observation[\"attempt\"].item()\n",
    "    if \"board\" in observation and observation[\"board\"] is not None:\n",
    "        board = nested_list_to_tuple(observation)\n",
    "    else:\n",
    "        board = tuple()\n",
    "        \n",
    "    if attempt == 0:\n",
    "        return \"initial\"\n",
    "        \n",
    "    return (attempt, board)\n",
    "\n",
    "def simulate_game_with_target(env, Q, target_word, log_dir=\"./logs/wordle_q_learning\"):\n",
    "    \"\"\"\n",
    "    Simulate a single game with the target word fixed to target_word with TensorBoard logging.\n",
    "    Uses the learned Q-values to choose actions. Returns the number of moves\n",
    "    taken to solve the word if successful, or 7 if the agent fails within 6 moves.\n",
    "    \"\"\"\n",
    "    writer = None\n",
    "    log_this_game = random.random() < 0.1\n",
    "    \n",
    "    if log_this_game:\n",
    "        writer = SummaryWriter(log_dir=f\"{log_dir}_{target_word}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    \n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    if hasattr(env, 'target'):\n",
    "        env.target = target_word\n",
    "    elif hasattr(env, 'target_word'):\n",
    "        env.target_word = target_word\n",
    "    \n",
    "    state = get_state(observation)\n",
    "    guessed_actions = set()\n",
    "    done = False\n",
    "    step = 0\n",
    "    guesses = []\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "        if not allowed_actions:\n",
    "            action = env.action_space.sample()\n",
    "        elif state in Q:\n",
    "            allowed_q = {a: Q[state][a] for a in allowed_actions if a in Q[state]}\n",
    "            if allowed_q:\n",
    "                action = max(allowed_q, key=allowed_q.get)\n",
    "            else:\n",
    "                action = random.choice(allowed_actions)\n",
    "        else:\n",
    "            action = random.choice(allowed_actions)\n",
    "            \n",
    "        guessed_actions.add(action)\n",
    "        \n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        if writer:\n",
    "            word = env.valid_words[action] if hasattr(env, 'valid_words') else f\"Action_{action}\"\n",
    "            guesses.append(word)\n",
    "            writer.add_scalar('Simulation/Step_Reward', reward, step)\n",
    "            \n",
    "        state = get_state(observation)\n",
    "    \n",
    "    success = False\n",
    "    if hasattr(env, 'won'):\n",
    "        success = env.won\n",
    "    elif hasattr(env, 'success'):\n",
    "        success = env.success\n",
    "    else:\n",
    "        success = total_reward > 0\n",
    "    \n",
    "    attempts = 0\n",
    "    if hasattr(env, 'attempt'):\n",
    "        attempts = env.attempt\n",
    "    else:\n",
    "        attempts = step\n",
    "    \n",
    "    result = attempts if success else 7\n",
    "    \n",
    "    if writer:\n",
    "        writer.add_scalar('Simulation/Total_Reward', total_reward, 0)\n",
    "        writer.add_scalar('Simulation/Success', 1 if success else 0, 0)\n",
    "        writer.add_scalar('Simulation/Attempts', attempts, 0)\n",
    "        \n",
    "        summary = f\"Target word: {target_word}\\n\"\n",
    "        summary += f\"Success: {success}\\n\"\n",
    "        summary += f\"Attempts: {attempts}\\n\"\n",
    "        summary += f\"Guesses: {', '.join(guesses)}\\n\"\n",
    "        summary += f\"Total reward: {total_reward}\"\n",
    "        \n",
    "        writer.add_text('Simulation/Game_Summary', summary, 0)\n",
    "        writer.close()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate_agent(env, Q, valid_words=None, num_words=100, log_dir=\"./logs/wordle_evaluation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the trained agent on multiple target words and log aggregate statistics.\n",
    "    \n",
    "    Args:\n",
    "        env: Wordle environment\n",
    "        Q: Trained Q-table\n",
    "        valid_words: List of target words to evaluate on (if None, uses random words)\n",
    "        num_words: Number of words to evaluate (if valid_words is None)\n",
    "        log_dir: Directory for TensorBoard logs\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    \n",
    "    if valid_words is None:\n",
    "        if hasattr(env, 'valid_words'):\n",
    "            valid_words = random.sample(env.valid_words, min(num_words, len(env.valid_words)))\n",
    "        else:\n",
    "            valid_words = [f\"word_{i}\" for i in range(num_words)]\n",
    "    \n",
    "    results = []\n",
    "    success_count = 0\n",
    "    attempts_by_success = []\n",
    "    \n",
    "    for i, target_word in enumerate(valid_words):\n",
    "        attempts = simulate_game_with_target(env, Q, target_word)\n",
    "        \n",
    "        success = attempts < 7  # 7 means failure\n",
    "        if success:\n",
    "            success_count += 1\n",
    "            attempts_by_success.append(attempts)\n",
    "        \n",
    "        results.append((target_word, attempts, success))\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Evaluated {i + 1}/{len(valid_words)} words | Success rate: {success_count/(i+1):.2f}\")\n",
    "        \n",
    "        writer.add_scalar('Evaluation/Success', 1 if success else 0, i)\n",
    "        writer.add_scalar('Evaluation/Attempts', attempts if attempts < 7 else 7, i)\n",
    "    \n",
    "    success_rate = success_count / len(valid_words)\n",
    "    avg_attempts = sum(attempts_by_success) / max(1, success_count)\n",
    "    \n",
    "    attempts_dist = {i: attempts_by_success.count(i) for i in range(1, 7)}\n",
    "    \n",
    "    writer.add_scalar('Evaluation/Overall_Success_Rate', success_rate, 0)\n",
    "    writer.add_scalar('Evaluation/Average_Attempts_When_Successful', avg_attempts, 0)\n",
    "    \n",
    "    for attempts, count in attempts_dist.items():\n",
    "        if count > 0:\n",
    "            percentage = count / max(1, success_count)\n",
    "            writer.add_scalar('Evaluation/Attempts_Distribution', percentage, attempts)\n",
    "    \n",
    "    writer.add_histogram('Evaluation/Attempts_Histogram', np.array(attempts_by_success), 0)\n",
    "    \n",
    "    summary = f\"Evaluation Results:\\n\"\n",
    "    summary += f\"Total words: {len(valid_words)}\\n\"\n",
    "    summary += f\"Success rate: {success_rate:.2f}\\n\"\n",
    "    summary += f\"Average attempts when successful: {avg_attempts:.2f}\\n\\n\"\n",
    "    summary += f\"Attempts distribution:\\n\"\n",
    "    for attempts, count in sorted(attempts_dist.items()):\n",
    "        percentage = count / max(1, success_count) * 100\n",
    "        summary += f\"  {attempts} attempts: {count} words ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    writer.add_text('Evaluation/Summary', summary, 0)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": success_rate,\n",
    "        \"avg_attempts\": avg_attempts,\n",
    "        \"attempts_distribution\": attempts_dist,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000 | Reward: 75.00 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.1218 | Q-states: 6\n",
      "Episode 200/1000 | Reward: -111.42 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0738 | Q-states: 6\n",
      "Episode 300/1000 | Reward: 230.76 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0447 | Q-states: 6\n",
      "Episode 400/1000 | Reward: 465.89 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0271 | Q-states: 6\n",
      "Episode 500/1000 | Reward: 234.97 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0164 | Q-states: 6\n",
      "Episode 600/1000 | Reward: 115.30 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 6\n",
      "Episode 700/1000 | Reward: 4.04 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 6\n",
      "Episode 800/1000 | Reward: -160.13 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 6\n",
      "Episode 900/1000 | Reward: -249.14 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 6\n",
      "Episode 1000/1000 | Reward: 394.93 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2309/2309 [00:08<00:00, 275.44it/s]\n"
     ]
    }
   ],
   "source": [
    "env = WordleEnv(word_list_path=\"target_words.txt\")\n",
    "Q = train_q_learning(env, num_episodes=1000)\n",
    "move_counts = []\n",
    "for target_word in tqdm(env.valid_words):\n",
    "    moves = simulate_game_with_target(env, Q, target_word)\n",
    "    move_counts.append(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning_finite_horizon(env, num_episodes=1000, alpha=0.5, gamma=0.9, epsilon=0.2, word_length=5, \n",
    "                                    num_letters=26, max_attempts=6, log_dir=\"./logs/wordle_q_learning_information\"):\n",
    "    \"\"\"\n",
    "    Finite-horizon Q-learning algorithm for Wordle, using an information state.\n",
    "    This implementation explicitly accounts for the remaining attempts in the Q-table.\n",
    "\n",
    "    Args:\n",
    "        env: The Wordle environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration rate\n",
    "        word_length: Length of the target word\n",
    "        num_letters: Number of possible letters (26 for English alphabet)\n",
    "        max_attempts: Maximum number of attempts allowed in Wordle (typically 6)\n",
    "        log_dir: Directory for TensorBoard logs\n",
    "\n",
    "    Returns:\n",
    "        Q: The learned Q-table, indexed by (state, attempts_remaining)\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    \n",
    "    Q = {}  # Q-table indexed by (state, attempts_remaining)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    win_rate_window = deque(maxlen=100)\n",
    "    q_values_history = []\n",
    "    exploration_rates = []\n",
    "    unique_states_count = []\n",
    "    \n",
    "    epsilon_start = epsilon\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    def get_information_state(observation):\n",
    "        \"\"\"\n",
    "        Extract a compact information state from the observation, mimicking WordleFeatureExtractor_Markov.\n",
    "        This version directly mirrors the logic of the `forward` method, adapted for a single environment.\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.zeros((word_length, num_letters))\n",
    "        greens = {}  # {letter_idx: [positions]}\n",
    "        yellows = {}  # {letter_idx: [positions]}\n",
    "        blacks = {}  # {letter_idx: [positions]}  for truly absent letters\n",
    "        missing_letters = {pos: [] for pos in range(word_length)}\n",
    "\n",
    "        attempt_idx = observation['attempt'].item()\n",
    "\n",
    "        if attempt_idx == 0:\n",
    "            return \"initial\"\n",
    "\n",
    "        for guess_idx in range(attempt_idx):\n",
    "            last_feedback = observation['board'][guess_idx]\n",
    "            last_guess = observation['guesses'][guess_idx]\n",
    "\n",
    "            if (last_guess < 0).any():\n",
    "                continue  \n",
    "            for idx, (feed, letter) in enumerate(zip(last_feedback, last_guess)):\n",
    "                letter_item = letter.item()\n",
    "                if feed == 2:  # Green\n",
    "                    if letter_item not in greens:\n",
    "                        greens[letter_item] = []\n",
    "                    greens[letter_item].append(idx)\n",
    "\n",
    "                elif feed == 1:  # Yellow\n",
    "                    if letter_item not in yellows:\n",
    "                        yellows[letter_item] = []\n",
    "                    yellows[letter_item].append(idx)\n",
    "                elif feed == 0:  # Black (Gray)\n",
    "                    if letter_item not in blacks:\n",
    "                        blacks[letter_item] = []\n",
    "                    blacks[letter_item].append(idx)\n",
    "\n",
    "        # Process green positions\n",
    "        for letter_idx, positions in greens.items():\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = 1\n",
    "                for other_letter in range(num_letters):\n",
    "                    if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "        # Process yellows *after* greens\n",
    "        for letter_idx, positions in yellows.items():\n",
    "\n",
    "            # Exclude yellows from the positions, blacks for that letter and greens from being candidates\n",
    "            candidate_positions = [p for p in range(word_length) if p not in positions and p not in greens.get(letter_idx,[]) and \n",
    "                                    (p not in blacks.get(letter_idx, []))]\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = -1\n",
    "                if letter_idx not in missing_letters[pos]:\n",
    "                    missing_letters[pos].append(letter_idx)\n",
    "\n",
    "            if candidate_positions:\n",
    "                yellow_value = min(1.0, len(positions) / len(candidate_positions))\n",
    "\n",
    "                for pos in candidate_positions:\n",
    "                  state[pos, letter_idx] = yellow_value\n",
    "                  if yellow_value == 1:  #yellow confirmed at position\n",
    "                    for other_letter in range(num_letters):\n",
    "                      if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "        # Process blacks *after* greens and yellows\n",
    "        for letter_idx, positions in blacks.items():\n",
    "            has_positive_info = (state[:, letter_idx] > 0).any()\n",
    "\n",
    "            if has_positive_info:\n",
    "                # If we have green or yellow info, just mark black positions as impossible\n",
    "                for pos in positions:\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                      missing_letters[pos].append(letter_idx)\n",
    "            else:\n",
    "                # No positive info, the letter is absent\n",
    "                for pos in range(word_length):\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                        missing_letters[pos].append(letter_idx)\n",
    "\n",
    "        return state.flatten().numpy().tobytes()\n",
    "\n",
    "    def choose_action(state, attempts_remaining, guessed_actions, current_epsilon):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state and attempts remaining, \n",
    "        using epsilon-greedy strategy.\n",
    "        \"\"\"\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "\n",
    "        if not allowed_actions:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        state_time_key = (state, attempts_remaining)\n",
    "        \n",
    "        if random.random() < current_epsilon or state_time_key not in Q:\n",
    "            return random.choice(allowed_actions), True\n",
    "        else:\n",
    "            q_values = Q[state_time_key]\n",
    "            allowed_q = {a: q_values[a] for a in allowed_actions}\n",
    "            return max(allowed_q, key=allowed_q.get), False\n",
    "\n",
    "    def update_Q(state, attempts_remaining, action, reward, next_state, next_attempts_remaining, done):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning update rule for finite-horizon MDPs.\n",
    "        The Q-value now depends on both state and time step (attempts remaining).\n",
    "        \"\"\"\n",
    "        state_time_key = (state, attempts_remaining)\n",
    "        next_state_time_key = (next_state, next_attempts_remaining)\n",
    "        \n",
    "        if state_time_key not in Q:\n",
    "            Q[state_time_key] = {a: 0 for a in range(env.action_space.n)}\n",
    "        \n",
    "        if not done and next_state_time_key not in Q:\n",
    "            Q[next_state_time_key] = {a: 0 for a in range(env.action_space.n)}\n",
    "\n",
    "        best_next_action_value = max(Q[next_state_time_key].values()) if not done else 0\n",
    "        Q[state_time_key][action] += alpha * (reward + gamma * best_next_action_value - Q[state_time_key][action])\n",
    "        \n",
    "        return Q[state_time_key][action]\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observation, _ = env.reset()\n",
    "        state = get_information_state(observation)\n",
    "        attempts_remaining = max_attempts\n",
    "        done = False\n",
    "        guessed_actions = set()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        exploration_count = 0\n",
    "        \n",
    "        current_epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "        \n",
    "        episode_q_values = []\n",
    "        \n",
    "        while not done:\n",
    "            action, is_exploration = choose_action(state, attempts_remaining, guessed_actions, current_epsilon)\n",
    "            guessed_actions.add(action)\n",
    "            if is_exploration:\n",
    "                exploration_count += 1\n",
    "            \n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = get_information_state(next_observation)\n",
    "            \n",
    "            next_attempts_remaining = attempts_remaining - 1\n",
    "            \n",
    "            new_q_value = update_Q(state, attempts_remaining, action, reward, next_state, \n",
    "                                   next_attempts_remaining, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_step += 1\n",
    "            episode_q_values.append(new_q_value)\n",
    "            \n",
    "            state = next_state\n",
    "            attempts_remaining = next_attempts_remaining\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_step)\n",
    "        win_rate_window.append(1 if env.won else 0)\n",
    "        exploration_rates.append(exploration_count / episode_step if episode_step > 0 else 0)\n",
    "        \n",
    "        avg_q_value = sum(episode_q_values) / len(episode_q_values) if episode_q_values else 0\n",
    "        q_values_history.append(avg_q_value)\n",
    "        \n",
    "        unique_states_count.append(len(Q))\n",
    "        \n",
    "        writer.add_scalar('Metrics/Reward', episode_reward, episode)\n",
    "        writer.add_scalar('Metrics/Episode_Length', episode_step, episode)\n",
    "        writer.add_scalar('Metrics/Win_Rate', sum(win_rate_window) / len(win_rate_window), episode)\n",
    "        writer.add_scalar('Metrics/Exploration_Rate', exploration_rates[-1], episode)\n",
    "        writer.add_scalar('Metrics/Average_Q_Value', avg_q_value, episode)\n",
    "        writer.add_scalar('Metrics/Unique_States', unique_states_count[-1], episode)\n",
    "        writer.add_scalar('Hyperparameters/Epsilon', current_epsilon, episode)\n",
    "        \n",
    "        if env.won and hasattr(env, 'target_word') and episode % 50 == 0:\n",
    "            writer.add_text('Examples/Won_Games', \n",
    "                           f\"Episode {episode}: Solved '{env.target_word}' in {episode_step} attempts\", \n",
    "                           episode)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            win_rate = sum(win_rate_window) / len(win_rate_window) if win_rate_window else 0\n",
    "            print(f\"Episode {episode}/{num_episodes} | Reward: {episode_reward:.2f} | \" \n",
    "                  f\"Steps: {episode_step} | Win Rate: {win_rate:.2f} | \"\n",
    "                  f\"Epsilon: {current_epsilon:.4f} | Q-states: {len(Q)}\")\n",
    "    \n",
    "    writer.add_histogram('Histograms/Episode_Rewards', np.array(episode_rewards), 0)\n",
    "    writer.add_histogram('Histograms/Episode_Lengths', np.array(episode_lengths), 0)\n",
    "    writer.add_histogram('Histograms/Q_Values', np.array(q_values_history), 0)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "def test_agent_finite_horizon(env, Q, num_test_episodes=100, word_length=5, num_letters=26, max_attempts=6, log_dir=\"./logs/wordle_test\"):\n",
    "    \"\"\"\n",
    "    Test the trained finite-horizon agent with TensorBoard logging.\n",
    "\n",
    "    Args:\n",
    "        env: The Wordle environment\n",
    "        Q: The learned Q-table indexed by (state, attempts_remaining)\n",
    "        num_test_episodes: Number of test episodes to run\n",
    "        word_length: Length of the target word\n",
    "        num_letters: Number of possible letters (26 for English alphabet)\n",
    "        max_attempts: Maximum number of attempts allowed in Wordle (typically 6)\n",
    "        log_dir: Directory for TensorBoard logs\n",
    "\n",
    "    Returns:\n",
    "        results: List of (target_word, num_attempts, success) tuples\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    \n",
    "    def get_information_state(observation):\n",
    "        \"\"\"\n",
    "        Extract a compact information state from the observation, mimicking WordleFeatureExtractor_Markov.\n",
    "        This version directly mirrors the logic of the `forward` method, adapted for a single environment.\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.zeros((word_length, num_letters))\n",
    "        greens = {} \n",
    "        yellows = {} \n",
    "        blacks = {}\n",
    "        missing_letters = {pos: [] for pos in range(word_length)}\n",
    "\n",
    "        attempt_idx = observation['attempt'].item()\n",
    "\n",
    "        if attempt_idx == 0:\n",
    "            return \"initial\" \n",
    "        for guess_idx in range(attempt_idx):\n",
    "            last_feedback = observation['board'][guess_idx]\n",
    "            last_guess = observation['guesses'][guess_idx]\n",
    "\n",
    "            if (last_guess < 0).any():\n",
    "                continue  \n",
    "            for idx, (feed, letter) in enumerate(zip(last_feedback, last_guess)):\n",
    "                letter_item = letter.item()\n",
    "                if feed == 2:\n",
    "                    if letter_item not in greens:\n",
    "                        greens[letter_item] = []\n",
    "                    greens[letter_item].append(idx)\n",
    "\n",
    "                elif feed == 1:\n",
    "                    if letter_item not in yellows:\n",
    "                        yellows[letter_item] = []\n",
    "                    yellows[letter_item].append(idx)\n",
    "                elif feed == 0:\n",
    "                    if letter_item not in blacks:\n",
    "                        blacks[letter_item] = []\n",
    "                    blacks[letter_item].append(idx)\n",
    "\n",
    "        for letter_idx, positions in greens.items():\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = 1\n",
    "                for other_letter in range(num_letters):\n",
    "                    if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "        for letter_idx, positions in yellows.items():\n",
    "\n",
    "            candidate_positions = [p for p in range(word_length) if p not in positions and p not in greens.get(letter_idx,[])]\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = -1\n",
    "                if letter_idx not in missing_letters[pos]:\n",
    "                    missing_letters[pos].append(letter_idx)\n",
    "\n",
    "            if candidate_positions:\n",
    "                yellow_value = min(1.0, len(positions) / len(candidate_positions))\n",
    "\n",
    "                for pos in candidate_positions:\n",
    "                  state[pos, letter_idx] = yellow_value\n",
    "                  if yellow_value == 1:\n",
    "                    for other_letter in range(num_letters):\n",
    "                      if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "        for letter_idx, positions in blacks.items():\n",
    "            has_positive_info = (state[:, letter_idx] > 0).any()\n",
    "\n",
    "            if has_positive_info:\n",
    "                for pos in positions:\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                      missing_letters[pos].append(letter_idx)\n",
    "            else:\n",
    "                for pos in range(word_length):\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                        missing_letters[pos].append(letter_idx)\n",
    "\n",
    "        return state.flatten().numpy().tobytes()\n",
    "    \n",
    "    results = []\n",
    "    total_reward = 0\n",
    "    wins = 0\n",
    "    attempts_distribution = {i: 0 for i in range(1, max_attempts + 1)}\n",
    "    \n",
    "    for episode in range(num_test_episodes):\n",
    "        observation, _ = env.reset()\n",
    "        target_word = env.target_word\n",
    "        state = get_information_state(observation)\n",
    "        attempts_remaining = max_attempts\n",
    "        done = False\n",
    "        guessed_actions = set()\n",
    "        attempts_used = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            attempts_used += 1\n",
    "            allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "            \n",
    "            state_time_key = (state, attempts_remaining)\n",
    "            \n",
    "            if not allowed_actions:\n",
    "                attempts_used = max_attempts\n",
    "                action = env.action_space.sample()\n",
    "                done = True\n",
    "            elif state_time_key in Q:\n",
    "                q_values = Q[state_time_key]\n",
    "                allowed_q = {a: q_values[a] for a in allowed_actions}\n",
    "                action = max(allowed_q, key=allowed_q.get)\n",
    "            else:\n",
    "                action = random.choice(allowed_actions)\n",
    "\n",
    "            guessed_actions.add(action)\n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = get_information_state(next_observation)\n",
    "            attempts_remaining -= 1\n",
    "            state = next_state\n",
    "\n",
    "        success = env.won\n",
    "        if success:\n",
    "            wins += 1\n",
    "            attempts_distribution[attempts_used] += 1\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "        results.append((target_word, attempts_used, success))\n",
    "        \n",
    "        writer.add_scalar('Test/Reward', episode_reward, episode)\n",
    "        writer.add_scalar('Test/Attempts', attempts_used, episode)\n",
    "        writer.add_scalar('Test/Success', 1 if success else 0, episode)\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Tested {episode} / {num_test_episodes} | Win rate: {wins/(episode+1):.2f}\")\n",
    "    \n",
    "    win_rate = wins / num_test_episodes\n",
    "    avg_attempts = sum(result[1] for result in results) / num_test_episodes\n",
    "    avg_reward = total_reward / num_test_episodes\n",
    "    \n",
    "    writer.add_scalar('Test/Overall_Win_Rate', win_rate, 0)\n",
    "    writer.add_scalar('Test/Average_Attempts', avg_attempts, 0)\n",
    "    writer.add_scalar('Test/Average_Reward', avg_reward, 0)\n",
    "    \n",
    "    for attempts, count in attempts_distribution.items():\n",
    "        if count > 0:\n",
    "            writer.add_scalar('Test/Success_Distribution', count / wins, attempts)\n",
    "    \n",
    "    for i, (word, attempts, success) in enumerate(results[:10]):\n",
    "        writer.add_text('Test/Examples', \n",
    "                       f\"Word: {word} | Attempts: {attempts} | Success: {success}\", \n",
    "                       i)\n",
    "    \n",
    "    writer.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000 | Reward: 119.50 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.2000 | Q-states: 6\n",
      "Episode 100/10000 | Reward: -66.92 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.1212 | Q-states: 389\n",
      "Episode 200/10000 | Reward: 257.22 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0734 | Q-states: 699\n",
      "Episode 300/10000 | Reward: -111.42 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0445 | Q-states: 995\n",
      "Episode 400/10000 | Reward: 212.71 | Steps: 6 | Win Rate: 0.01 | Epsilon: 0.0269 | Q-states: 1230\n",
      "Episode 500/10000 | Reward: -40.46 | Steps: 6 | Win Rate: 0.02 | Epsilon: 0.0163 | Q-states: 1487\n",
      "Episode 600/10000 | Reward: 141.76 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 1701\n",
      "Episode 700/10000 | Reward: -89.17 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 1987\n",
      "Episode 800/10000 | Reward: -182.38 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 2228\n",
      "Episode 900/10000 | Reward: -204.63 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 2420\n",
      "Episode 1000/10000 | Reward: 48.54 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 2602\n",
      "Episode 1100/10000 | Reward: 75.00 | Steps: 6 | Win Rate: 0.02 | Epsilon: 0.0100 | Q-states: 2762\n",
      "Episode 1200/10000 | Reward: 279.47 | Steps: 6 | Win Rate: 0.01 | Epsilon: 0.0100 | Q-states: 2984\n",
      "Episode 1300/10000 | Reward: 283.67 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 3173\n",
      "Episode 1400/10000 | Reward: 190.46 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 3388\n",
      "Episode 1500/10000 | Reward: 26.29 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 3580\n",
      "Episode 1600/10000 | Reward: 48.54 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 3756\n",
      "Episode 1700/10000 | Reward: 97.25 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 3923\n",
      "Episode 1800/10000 | Reward: 48.54 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 4100\n",
      "Episode 1900/10000 | Reward: 4.04 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 4301\n",
      "Episode 2000/10000 | Reward: -18.21 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 4514\n",
      "Episode 2100/10000 | Reward: 234.97 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 4685\n",
      "Episode 2200/10000 | Reward: 230.76 | Steps: 6 | Win Rate: 0.00 | Epsilon: 0.0100 | Q-states: 4817\n"
     ]
    }
   ],
   "source": [
    "env = WordleEnvMarkov(\"target_words.txt\")\n",
    "Q = train_q_learning_finite_horizon(env, num_episodes=10000)\n",
    "\n",
    "results = test_agent_finite_horizon(env, Q, num_test_episodes=100)\n",
    "\n",
    "total_episodes = len(results)\n",
    "successful_episodes = sum(1 for _, _, success in results if success)\n",
    "success_rate = successful_episodes / total_episodes\n",
    "print(f\"Success rate: {success_rate:.4f}\")\n",
    "\n",
    "move_counts = [attempts for _, attempts, success in results if success]\n",
    "if move_counts:\n",
    "    average_moves = sum(move_counts) / len(move_counts)\n",
    "    print(f\"Average moves for successful games: {average_moves:.2f}\")\n",
    "else:\n",
    "    print(\"No successful games to calculate average moves.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
