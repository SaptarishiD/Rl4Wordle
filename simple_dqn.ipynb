{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from environments import WordleEnv, WordleEnvMarkov\n",
    "\n",
    "# -----------------------------\n",
    "# Q-Learning Agent Definition\n",
    "# -----------------------------\n",
    "def train_q_learning(env, num_episodes=1000, alpha=0.5, gamma=0.9, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    A simple tabular Q-learning algorithm that trains on the Wordle environment.\n",
    "    \n",
    "    The state is defined as a tuple: (attempt_number, last_feedback)\n",
    "    where last_feedback is a tuple of length word_length.\n",
    "    \n",
    "    This version prevents the agent from guessing the same word twice in a single episode.\n",
    "    \"\"\"\n",
    "    Q = {}  # Q-table\n",
    "\n",
    "    def get_state(observation):\n",
    "        # Extract Information state from observation\n",
    "        return ()\n",
    "    \n",
    "    def choose_action(state, guessed_actions):\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "        if not allowed_actions:\n",
    "            return env.action_space.sample()\n",
    "        if random.random() < epsilon or state not in Q:\n",
    "            return random.choice(allowed_actions)\n",
    "        else:\n",
    "            q_values = Q[state]\n",
    "            allowed_q = {a: q_values[a] for a in allowed_actions}\n",
    "            return max(allowed_q, key=allowed_q.get)\n",
    "    \n",
    "    def update_Q(state, action, reward, next_state, done):\n",
    "        if state not in Q:\n",
    "            Q[state] = {a: 0 for a in range(env.action_space.n)}\n",
    "        if next_state not in Q:\n",
    "            Q[next_state] = {a: 0 for a in range(env.action_space.n)}\n",
    "        best_next = max(Q[next_state].values())\n",
    "        Q[state][action] += alpha * (reward + gamma * best_next * (1 - int(done)) - Q[state][action])\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observation, _ = env.reset()\n",
    "        state = get_state(observation)\n",
    "        guessed_actions = set() \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = choose_action(state, guessed_actions)\n",
    "            guessed_actions.add(action)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            print(\"Episode: \", episode, \"Action: \", action, \"Reward: \", reward, \"Done: \", done)\n",
    "            next_state = get_state(next_observation)\n",
    "            update_Q(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "        # Optional: print progress every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "    return Q\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Testing the Trained Agent\n",
    "# -----------------------------\n",
    "def test_agent(env, Q):\n",
    "    observation, _ = env.reset()\n",
    "    state = (observation[\"attempt\"], tuple(observation[\"feedback\"].tolist()))\n",
    "    guessed_actions = set()\n",
    "    done = False\n",
    "    print(\"\\nTesting trained agent:\")\n",
    "    while not done:\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "        if not allowed_actions:\n",
    "            # Fallback if all actions have been guessed.\n",
    "            action = env.action_space.sample()\n",
    "        elif state in Q:\n",
    "            # Choose the best allowed action based on Q-values.\n",
    "            allowed_q = {a: Q[state][a] for a in allowed_actions}\n",
    "            action = max(allowed_q, key=allowed_q.get)\n",
    "        else:\n",
    "            action = random.choice(allowed_actions)\n",
    "        guessed_actions.add(action)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        print(f\"Guess: {env.word_list[action]}, Feedback: {observation['feedback']}, Reward: {reward}\")\n",
    "        state = (observation[\"attempt\"], tuple(observation[\"feedback\"].tolist()))\n",
    "    env.render()\n",
    "\n",
    "def simulate_game_with_target(env, Q, target_word):\n",
    "    \"\"\"\n",
    "    Simulate a single game with the target word fixed to target_word.\n",
    "    Uses the learned Q-values to choose actions. Returns the number of moves\n",
    "    taken to solve the word if successful, or 7 if the agent fails within 6 moves.\n",
    "    \"\"\"\n",
    "    observation, _ = env.reset()\n",
    "    # Override the randomly chosen target with the given target_word.\n",
    "    env.target = target_word\n",
    "    state = (observation[\"attempt\"], tuple(observation[\"feedback\"].tolist()))\n",
    "    guessed_actions = set()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "        if not allowed_actions:\n",
    "            action = env.action_space.sample()\n",
    "        elif state in Q:\n",
    "            allowed_q = {a: Q[state][a] for a in allowed_actions}\n",
    "            action = max(allowed_q, key=allowed_q.get)\n",
    "        else:\n",
    "            action = random.choice(allowed_actions)\n",
    "        guessed_actions.add(action)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        state = (observation[\"attempt\"], tuple(observation[\"feedback\"].tolist()))\n",
    "    # If reward is positive, the agent solved the word; otherwise record 7 moves (failure).\n",
    "    return env.attempt if reward > 0 else 7\n",
    "\n",
    "\n",
    "def plot_histogram(move_counts):\n",
    "    \"\"\"\n",
    "    Plots a histogram with 7 bins: moves 1 through 6 and 7 for failures.\n",
    "    The x-axis shows the number of moves and the y-axis the count of words.\n",
    "    \"\"\"\n",
    "    # Create bins labeled 1 through 7.\n",
    "    bins = np.arange(1, 9) - 0.5  # bin edges for 7 bins\n",
    "    plt.hist(move_counts, bins=bins, edgecolor=\"black\")\n",
    "    plt.xlabel(\"Number of Moves (7 = failure)\")\n",
    "    plt.ylabel(\"Number of Words\")\n",
    "    plt.title(\"Histogram of Moves Required to Solve Wordle\")\n",
    "    plt.xticks(range(1, 8))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = WordleEnv(word_list_file=\"target_words.txt\")\n",
    "# # Train the Q-learning agent.\n",
    "# Q = train_q_learning(env, num_episodes=1000)\n",
    "# # Test the trained agent.\n",
    "# # test_agent(env, Q)\n",
    "#     # Evaluate the agent on every possible target word.\n",
    "# move_counts = []\n",
    "# for target_word in tqdm(env.word_list):\n",
    "#     # For each target, reset the environment and simulate a game.\n",
    "#     moves = simulate_game_with_target(env, Q, target_word)\n",
    "#     # print(f\"Target: {target_word}, Moves: {moves}\")\n",
    "#     move_counts.append(moves)\n",
    "# print(Counter(move_counts).keys())\n",
    "# print(Counter(move_counts).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def train_q_learning_finite_horizon(env, num_episodes=1000, alpha=0.5, gamma=0.9, epsilon=0.2, word_length=5, num_letters=26, max_attempts=6):\n",
    "    \"\"\"\n",
    "    Finite-horizon Q-learning algorithm for Wordle, using an information state.\n",
    "    This implementation explicitly accounts for the remaining attempts in the Q-table.\n",
    "\n",
    "    Args:\n",
    "        env: The Wordle environment\n",
    "        num_episodes: Number of episodes to train\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration rate\n",
    "        word_length: Length of the target word\n",
    "        num_letters: Number of possible letters (26 for English alphabet)\n",
    "        max_attempts: Maximum number of attempts allowed in Wordle (typically 6)\n",
    "\n",
    "    Returns:\n",
    "        Q: The learned Q-table, indexed by (state, attempts_remaining)\n",
    "    \"\"\"\n",
    "    Q = {}  # Q-table indexed by (state, attempts_remaining)\n",
    "\n",
    "    def get_information_state(observation):\n",
    "        \"\"\"\n",
    "        Extract a compact information state from the observation, mimicking WordleFeatureExtractor_Markov.\n",
    "        This version directly mirrors the logic of the `forward` method, adapted for a single environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize state representation\n",
    "        state = torch.zeros((word_length, num_letters))\n",
    "        greens = {}  # {letter_idx: [positions]}\n",
    "        yellows = {}  # {letter_idx: [positions]}\n",
    "        blacks = {}  # {letter_idx: [positions]}  for truly absent letters\n",
    "        missing_letters = {pos: [] for pos in range(word_length)}\n",
    "\n",
    "        attempt_idx = observation['attempt'].item()\n",
    "\n",
    "        if attempt_idx == 0:\n",
    "            return \"initial\" # Using same string as original q learning impl\n",
    "\n",
    "        for guess_idx in range(attempt_idx):\n",
    "            last_feedback = observation['board'][guess_idx]\n",
    "            last_guess = observation['guesses'][guess_idx]\n",
    "\n",
    "            if (last_guess < 0).any():\n",
    "                continue  # Skip invalid guesses\n",
    "\n",
    "            for idx, (feed, letter) in enumerate(zip(last_feedback, last_guess)):\n",
    "                letter_item = letter.item()\n",
    "                if feed == 2:  # Green\n",
    "                    if letter_item not in greens:\n",
    "                        greens[letter_item] = []\n",
    "                    greens[letter_item].append(idx)\n",
    "\n",
    "                elif feed == 1:  # Yellow\n",
    "                    if letter_item not in yellows:\n",
    "                        yellows[letter_item] = []\n",
    "                    yellows[letter_item].append(idx)\n",
    "                elif feed == 0:  # Black (Gray)\n",
    "                    if letter_item not in blacks:\n",
    "                        blacks[letter_item] = []\n",
    "                    blacks[letter_item].append(idx)\n",
    "\n",
    "        # Process green positions\n",
    "        for letter_idx, positions in greens.items():\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = 1\n",
    "                for other_letter in range(num_letters):\n",
    "                    if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "         # Process yellows *after* greens\n",
    "        for letter_idx, positions in yellows.items():\n",
    "\n",
    "            # If the letter has been confirmed as green, adjust yellow processing.\n",
    "            confirmed_greens_count = len(greens.get(letter_idx, []))\n",
    "            \n",
    "            # Exclude yellows from the positions and greens from being candidates\n",
    "            candidate_positions = [p for p in range(word_length) if p not in positions and p not in greens.get(letter_idx,[])]\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = -1\n",
    "                if letter_idx not in missing_letters[pos]:\n",
    "                    missing_letters[pos].append(letter_idx)\n",
    "\n",
    "            if candidate_positions:\n",
    "                yellow_value = min(1.0, len(positions) / len(candidate_positions))\n",
    "\n",
    "                for pos in candidate_positions:\n",
    "                  state[pos, letter_idx] = yellow_value\n",
    "                  if yellow_value == 1:  #yellow confirmed at position\n",
    "                    for other_letter in range(num_letters):\n",
    "                      if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "        # Process blacks *after* greens and yellows\n",
    "        for letter_idx, positions in blacks.items():\n",
    "            has_positive_info = (state[:, letter_idx] > 0).any()\n",
    "\n",
    "            if has_positive_info:\n",
    "                # If we have green or yellow info, just mark black positions as impossible\n",
    "                for pos in positions:\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                      missing_letters[pos].append(letter_idx)\n",
    "            else:\n",
    "                # No positive info, the letter is likely absent\n",
    "                for pos in range(word_length):\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                        missing_letters[pos].append(letter_idx)\n",
    "\n",
    "        # Convert state to string representation for Q-table\n",
    "        return state.flatten().numpy().tobytes()\n",
    "\n",
    "    def choose_action(state, attempts_remaining, guessed_actions):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state and attempts remaining, \n",
    "        using epsilon-greedy strategy.\n",
    "        \"\"\"\n",
    "        allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "\n",
    "        if not allowed_actions:\n",
    "            return env.action_space.sample()  # All actions were tried\n",
    "\n",
    "        state_time_key = (state, attempts_remaining)\n",
    "        \n",
    "        if random.random() < epsilon or state_time_key not in Q:\n",
    "            return random.choice(allowed_actions)\n",
    "        else:\n",
    "            q_values = Q[state_time_key]\n",
    "            allowed_q = {a: q_values[a] for a in allowed_actions}\n",
    "            return max(allowed_q, key=allowed_q.get)\n",
    "\n",
    "    def update_Q(state, attempts_remaining, action, reward, next_state, next_attempts_remaining, done):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning update rule for finite-horizon MDPs.\n",
    "        The Q-value now depends on both state and time step (attempts remaining).\n",
    "        \"\"\"\n",
    "        state_time_key = (state, attempts_remaining)\n",
    "        next_state_time_key = (next_state, next_attempts_remaining)\n",
    "        \n",
    "        if state_time_key not in Q:\n",
    "            Q[state_time_key] = {a: 0 for a in range(env.action_space.n)}\n",
    "        \n",
    "        if not done and next_state_time_key not in Q:\n",
    "            Q[next_state_time_key] = {a: 0 for a in range(env.action_space.n)}\n",
    "\n",
    "        best_next_action_value = max(Q[next_state_time_key].values()) if not done else 0\n",
    "        Q[state_time_key][action] += alpha * (reward + gamma * best_next_action_value - Q[state_time_key][action])\n",
    "\n",
    "    # Main training loop\n",
    "    for episode in range(num_episodes):\n",
    "        observation, _ = env.reset()\n",
    "        state = get_information_state(observation)\n",
    "        attempts_remaining = max_attempts\n",
    "        done = False\n",
    "        guessed_actions = set()\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, attempts_remaining, guessed_actions)\n",
    "            guessed_actions.add(action)\n",
    "            \n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = get_information_state(next_observation)\n",
    "            \n",
    "            # Decrement attempts_remaining for the next state\n",
    "            next_attempts_remaining = attempts_remaining - 1\n",
    "            \n",
    "            update_Q(state, attempts_remaining, action, reward, next_state, next_attempts_remaining, done)\n",
    "            \n",
    "            state = next_state\n",
    "            attempts_remaining = next_attempts_remaining\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes} completed\")\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def test_agent_finite_horizon(env, Q, num_test_episodes=100, word_length=5, num_letters=26, max_attempts=6):\n",
    "    \"\"\"\n",
    "    Test the trained finite-horizon agent.\n",
    "\n",
    "    Args:\n",
    "        env: The Wordle environment\n",
    "        Q: The learned Q-table indexed by (state, attempts_remaining)\n",
    "        num_test_episodes: Number of test episodes to run\n",
    "        word_length: Length of the target word\n",
    "        num_letters: Number of possible letters (26 for English alphabet)\n",
    "        max_attempts: Maximum number of attempts allowed in Wordle (typically 6)\n",
    "\n",
    "    Returns:\n",
    "        results: List of (target_word, num_attempts, success) tuples\n",
    "    \"\"\"\n",
    "    def get_information_state(observation):\n",
    "        \"\"\"\n",
    "        Extract a compact information state from the observation, mimicking WordleFeatureExtractor_Markov.\n",
    "        This version directly mirrors the logic of the `forward` method, adapted for a single environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize state representation\n",
    "        state = torch.zeros((word_length, num_letters))\n",
    "        greens = {}  # {letter_idx: [positions]}\n",
    "        yellows = {}  # {letter_idx: [positions]}\n",
    "        blacks = {}  # {letter_idx: [positions]}  for truly absent letters\n",
    "        missing_letters = {pos: [] for pos in range(word_length)}\n",
    "\n",
    "        attempt_idx = observation['attempt'].item()\n",
    "\n",
    "        if attempt_idx == 0:\n",
    "            return \"initial\" # Using same string as original q learning impl\n",
    "\n",
    "        for guess_idx in range(attempt_idx):\n",
    "            last_feedback = observation['board'][guess_idx]\n",
    "            last_guess = observation['guesses'][guess_idx]\n",
    "\n",
    "            if (last_guess < 0).any():\n",
    "                continue  # Skip invalid guesses\n",
    "\n",
    "            for idx, (feed, letter) in enumerate(zip(last_feedback, last_guess)):\n",
    "                letter_item = letter.item()\n",
    "                if feed == 2:  # Green\n",
    "                    if letter_item not in greens:\n",
    "                        greens[letter_item] = []\n",
    "                    greens[letter_item].append(idx)\n",
    "\n",
    "                elif feed == 1:  # Yellow\n",
    "                    if letter_item not in yellows:\n",
    "                        yellows[letter_item] = []\n",
    "                    yellows[letter_item].append(idx)\n",
    "                elif feed == 0:  # Black (Gray)\n",
    "                    if letter_item not in blacks:\n",
    "                        blacks[letter_item] = []\n",
    "                    blacks[letter_item].append(idx)\n",
    "\n",
    "        # Process green positions\n",
    "        for letter_idx, positions in greens.items():\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = 1\n",
    "                for other_letter in range(num_letters):\n",
    "                    if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "         # Process yellows *after* greens\n",
    "        for letter_idx, positions in yellows.items():\n",
    "\n",
    "            # Exclude yellows from the positions and greens from being candidates\n",
    "            candidate_positions = [p for p in range(word_length) if p not in positions and p not in greens.get(letter_idx,[])]\n",
    "            for pos in positions:\n",
    "                state[pos, letter_idx] = -1\n",
    "                if letter_idx not in missing_letters[pos]:\n",
    "                    missing_letters[pos].append(letter_idx)\n",
    "\n",
    "            if candidate_positions:\n",
    "                yellow_value = min(1.0, len(positions) / len(candidate_positions))\n",
    "\n",
    "                for pos in candidate_positions:\n",
    "                  state[pos, letter_idx] = yellow_value\n",
    "                  if yellow_value == 1:  #yellow confirmed at position\n",
    "                    for other_letter in range(num_letters):\n",
    "                      if other_letter != letter_idx:\n",
    "                        state[pos, other_letter] = -1\n",
    "\n",
    "        # Process blacks *after* greens and yellows\n",
    "        for letter_idx, positions in blacks.items():\n",
    "            has_positive_info = (state[:, letter_idx] > 0).any()\n",
    "\n",
    "            if has_positive_info:\n",
    "                # If we have green or yellow info, just mark black positions as impossible\n",
    "                for pos in positions:\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                      missing_letters[pos].append(letter_idx)\n",
    "            else:\n",
    "                # No positive info, the letter is likely absent\n",
    "                for pos in range(word_length):\n",
    "                    state[pos, letter_idx] = -1\n",
    "                    if letter_idx not in missing_letters[pos]:\n",
    "                        missing_letters[pos].append(letter_idx)\n",
    "\n",
    "        # Convert state to string representation for Q-table\n",
    "        return state.flatten().numpy().tobytes()\n",
    "    \n",
    "    results = []\n",
    "    for episode in range(num_test_episodes):\n",
    "        observation, _ = env.reset()\n",
    "        target_word = env.target_word\n",
    "        state = get_information_state(observation)\n",
    "        attempts_remaining = max_attempts\n",
    "        done = False\n",
    "        guessed_actions = set()\n",
    "        attempts_used = 0\n",
    "\n",
    "        while not done:\n",
    "            attempts_used += 1\n",
    "            allowed_actions = [a for a in range(env.action_space.n) if a not in guessed_actions]\n",
    "            \n",
    "            state_time_key = (state, attempts_remaining)\n",
    "            \n",
    "            if not allowed_actions:\n",
    "                # If no actions are left for some reason, force it to quit out\n",
    "                attempts_used = max_attempts\n",
    "                action = env.action_space.sample()\n",
    "                done = True\n",
    "            elif state_time_key in Q:\n",
    "                # Find the best action among the allowed ones based on current state and attempts remaining\n",
    "                q_values = Q[state_time_key]\n",
    "                allowed_q = {a: q_values[a] for a in allowed_actions}\n",
    "                action = max(allowed_q, key=allowed_q.get)\n",
    "            else:\n",
    "                # If we haven't seen this state-time combination before, choose randomly\n",
    "                action = random.choice(allowed_actions)\n",
    "\n",
    "            guessed_actions.add(action)\n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = get_information_state(next_observation)\n",
    "            attempts_remaining -= 1\n",
    "            state = next_state\n",
    "\n",
    "        success = env.won\n",
    "        results.append((target_word, attempts_used, success))\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Tested {episode} / {num_test_episodes}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000 completed\n",
      "Episode 100/10000 completed\n",
      "Episode 200/10000 completed\n",
      "Episode 300/10000 completed\n",
      "Episode 400/10000 completed\n",
      "Episode 500/10000 completed\n",
      "Episode 600/10000 completed\n",
      "Episode 700/10000 completed\n",
      "Episode 800/10000 completed\n",
      "Episode 900/10000 completed\n",
      "Episode 1000/10000 completed\n",
      "Episode 1100/10000 completed\n",
      "Episode 1200/10000 completed\n",
      "Episode 1300/10000 completed\n",
      "Episode 1400/10000 completed\n",
      "Episode 1500/10000 completed\n",
      "Episode 1600/10000 completed\n",
      "Episode 1700/10000 completed\n",
      "Episode 1800/10000 completed\n",
      "Episode 1900/10000 completed\n",
      "Episode 2000/10000 completed\n",
      "Episode 2100/10000 completed\n",
      "Episode 2200/10000 completed\n",
      "Episode 2300/10000 completed\n",
      "Episode 2400/10000 completed\n",
      "Episode 2500/10000 completed\n",
      "Episode 2600/10000 completed\n",
      "Episode 2700/10000 completed\n",
      "Episode 2800/10000 completed\n",
      "Episode 2900/10000 completed\n",
      "Episode 3000/10000 completed\n",
      "Episode 3100/10000 completed\n",
      "Episode 3200/10000 completed\n",
      "Episode 3300/10000 completed\n",
      "Episode 3400/10000 completed\n",
      "Episode 3500/10000 completed\n",
      "Episode 3600/10000 completed\n",
      "Episode 3700/10000 completed\n",
      "Episode 3800/10000 completed\n",
      "Episode 3900/10000 completed\n",
      "Episode 4000/10000 completed\n",
      "Episode 4100/10000 completed\n",
      "Episode 4200/10000 completed\n",
      "Episode 4300/10000 completed\n",
      "Episode 4400/10000 completed\n",
      "Episode 4500/10000 completed\n",
      "Episode 4600/10000 completed\n",
      "Episode 4700/10000 completed\n",
      "Episode 4800/10000 completed\n",
      "Episode 4900/10000 completed\n",
      "Episode 5000/10000 completed\n",
      "Episode 5100/10000 completed\n",
      "Episode 5200/10000 completed\n",
      "Episode 5300/10000 completed\n",
      "Episode 5400/10000 completed\n",
      "Episode 5500/10000 completed\n",
      "Episode 5600/10000 completed\n",
      "Episode 5700/10000 completed\n",
      "Episode 5800/10000 completed\n",
      "Episode 5900/10000 completed\n",
      "Episode 6000/10000 completed\n",
      "Episode 6100/10000 completed\n",
      "Episode 6200/10000 completed\n",
      "Episode 6300/10000 completed\n",
      "Episode 6400/10000 completed\n",
      "Episode 6500/10000 completed\n",
      "Episode 6600/10000 completed\n",
      "Episode 6700/10000 completed\n",
      "Episode 6800/10000 completed\n",
      "Episode 6900/10000 completed\n",
      "Episode 7000/10000 completed\n",
      "Episode 7100/10000 completed\n",
      "Episode 7200/10000 completed\n",
      "Episode 7300/10000 completed\n",
      "Episode 7400/10000 completed\n",
      "Episode 7500/10000 completed\n",
      "Episode 7600/10000 completed\n",
      "Episode 7700/10000 completed\n",
      "Episode 7800/10000 completed\n",
      "Episode 7900/10000 completed\n",
      "Episode 8000/10000 completed\n",
      "Episode 8100/10000 completed\n",
      "Episode 8200/10000 completed\n",
      "Episode 8300/10000 completed\n",
      "Episode 8400/10000 completed\n",
      "Episode 8500/10000 completed\n",
      "Episode 8600/10000 completed\n",
      "Episode 8700/10000 completed\n",
      "Episode 8800/10000 completed\n",
      "Episode 8900/10000 completed\n",
      "Episode 9000/10000 completed\n",
      "Episode 9100/10000 completed\n",
      "Episode 9200/10000 completed\n",
      "Episode 9300/10000 completed\n",
      "Episode 9400/10000 completed\n",
      "Episode 9500/10000 completed\n",
      "Episode 9600/10000 completed\n",
      "Episode 9700/10000 completed\n",
      "Episode 9800/10000 completed\n",
      "Episode 9900/10000 completed\n"
     ]
    }
   ],
   "source": [
    "env = WordleEnvMarkov(\"target_words.txt\")\n",
    "Q = train_q_learning_finite_horizon(env, num_episodes=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 0 / 100\n",
      "Tested 20 / 100\n",
      "Tested 40 / 100\n",
      "Tested 60 / 100\n",
      "Tested 80 / 100\n",
      "Success rate: 0.0000\n",
      "No successful games to calculate average moves.\n"
     ]
    }
   ],
   "source": [
    "results = test_agent_finite_horizon(env, Q, num_test_episodes=100)\n",
    "\n",
    "total_episodes = len(results)\n",
    "successful_episodes = sum(1 for _, _, success in results if success)\n",
    "success_rate = successful_episodes / total_episodes\n",
    "print(f\"Success rate: {success_rate:.4f}\")\n",
    "\n",
    "move_counts = [attempts for _, attempts, success in results if success]\n",
    "if move_counts:\n",
    "    average_moves = sum(move_counts) / len(move_counts)\n",
    "    print(f\"Average moves for successful games: {average_moves:.2f}\")\n",
    "else:\n",
    "    print(\"No successful games to calculate average moves.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
